#!/usr/bin/env python3
import hashlib
import os
import re
import sys
import subprocess

FILE_LIST = "notebook.tex"
OUT_NAME = "out"


def checksum(line):
    """Calculates the checksum consistently with /src/check.sh."""
    numchars = 2
    line.strip()
    if len(line) == 0:  # Print nothing for an empty line
        return ""

    line = re.sub(r"\s*|//.*", r"", line)
    if len(line) == 0:  # Indent a comment-only line properly
        return " " * numchars

    return hashlib.md5(bytes(line + "\n", "utf8")).hexdigest()[:numchars]


def readfile(fn):
    with open(fn) as f:
        return f.read()


def highlight(text):
    def sp(begin, end, name, t):
        """Highlight the first occurence of [begin]text[end] in text."""
        if begin not in t:
            return None

        pre, post = t.split(begin, 1)
        post = post.split(end, 1)
        if len(post) == 1:
            post.append("")

        pre, text, post = pre, post[0], post[1]

        # The recursion level will be at most 5
        return (highlight(pre) + r"\%s{%s%s%s}" % (name, begin, text, end), post)

    l = []
    while True:
        # The order is important: /* "string" */ will be highlighted as a comment
        # TODO: preprocessor directives should be ^#.*, but we don't support regexes here
        for b, e, n in [("//", "\n", "com"), ("/*", "*/", "com"), ("#", "\n", "dir"), ('"', '"', "str"), ("'", "'", "str")]:
            res = sp(b, e, n, text)
            if res is not None:
                l.append(res[0])
                text = res[1]
                break
        else:
            break

    def repl(words, name, text):
        return re.sub(r"(^|\W)(%s)((?= )|(?=>)|(?=,)|$|\W)" % words, r"\1\\%s{\2}\3" % (name), text)

    # Types, templated types, keywords, constants
    d = {
        "typ": "bool char signed unsigned short long int ll ld float double void auto string ull",
        "tem": "array vector deque forward_list list set map multiset multimap unordered_set unordered_map unordered_multiset unordered_multimap stack queue priority_queue pair",
        "key": "break case class const continue delete do else for friend goto if inline namespace new private public return static struct switch this typedef using while rep per FOR",
        "con": "true false PI pi INF inf EPS eps NULL MOD MAXN MAXM"
    }

    for name in d.keys():
        text = repl(d[name].replace(" ", "|"), name, text)

    text = re.sub(r"(^|\W)(-?\d+(U?L?L|(\.\d+)?(e-?\d+)?))((?= )|$|\W)", r"\1\\con{\2}\6", text)
    l.append(text)
    return "".join(l)


def highlight_and_texify(text):
    for i in [("\\", r"\\"), ("{", r"\{"), ("}", r"\}"), ("\n\\}\n\n", "\n\\}\n\\greatsk\n"), ("\n\n", "\n\\sk\n")]:
        text = text.replace(i[0], i[1])

    return highlight(text)


def make_header(header, processed_filename):
    """Extracts relevant information from the header, returns TeX code."""
    def texify(text):
        """A set of hacks trying to convert most common shortcuts in the kactl
        headers (not entering math mode when describing complexities)."""
        text = text.replace("$O", r"$\O").replace("*", "\cdot ")
        text = re.sub(r"^O\((.*?)\)", r"$\O(\1)$", text)
        return text

    # Other keywords are silently discarded
    keywords = ["Description", "Time", "Memory", "Usage", "Name"]

    l = []
    heading = r"\ttheader{%s}" % processed_filename
    for h in keywords:
        if h not in header:
            continue

        if h == "Usage":
            # TODO: quite a hack
            l.append(r"{\leftskip=\usagedimen\item{\bf Usage:}\raggedright\simpleverb|%s\egroup\par}" % highlight_and_texify(header[h]))
        elif h == "Name":
            heading = r"\header{%s}" % header[h]
        else:
            l.append(r"{\bf %s:} %s" % (h, texify(header[h])))

    l = [heading] + l
    return "\n\n".join(l)


def process_file(f, chdir="../src"):
    def process(text):
        # Convert to two-spaced indent, assuming all of the original code is
        # indented by tabs or four spaces
        text = text.replace("\t", "    ")
        text = re.sub("^(  )+", " ", text)

        header, text = preprocess(text)
        text = re.sub("^ +$", "", text)
        checksums = [checksum(line) for line in text.split("\n")] + [""]
        highlighted = highlight_and_texify(text)

        def puttogether(hash, line):
            return r"\h{%s}  %s" % (hash, line) if hash else line

        return header, "\n".join(puttogether(*a) for a in zip(checksums, highlighted.split("\n"))).strip()

    def sanitize(text):
        return text.replace("_", r"\_")

    def work():
        header, text = process(readfile(chdir + "/" + f))
        header_text = make_header(header, sanitize(f))
        if header_text:
            yield header_text
        yield "\n\\smallskip\\verb|" + text
        yield "\egroup"

    print(f)
    return "\n".join(work())


def main():
    os.chdir(os.path.realpath(os.path.dirname(sys.argv[0])))
    text = readfile(FILE_LIST).strip()
    text = re.sub(r"(?<=\n)\[\[(.*?)\]\]", lambda a: process_file(a.group(1)), text)
    with open("%s.tex" % OUT_NAME, "w") as f:
        f.write(text)

    # Two runs are needed because TOC needs to be created at all, the third one
    # is for correct page numbers in TOC (because the TOC itself may cause
    # things to appear one page later than previously)
    for i in range(3):
        exit_code = subprocess.call(["xetex", OUT_NAME])
        if exit_code:
            break

    return exit_code


def parse_header(text):
    """Parses /** ... */ headers. Returns a tuple of Key: Content pairs."""
    if not text.startswith("/**\n"):
        return None
    header = text.split("/**\n", 1)[1].split("\n */", 1)[0]
    key = None
    d = {}
    LINE_START = " * "
    for line in header.split("\n"):
        if line.startswith(LINE_START):
            line = line.split(LINE_START, 1)[1]

        if line[0] != " ":
            # New field, eat the name and reduce it to the "line continuation" case (below)
            line = line.split(":", 1)
            if len(line) != 2:
                raise Exception("Expected field name ( * Something: ) preceeding the content: {}".format(line))

            key = line[0]
            if key in d:
                raise Exception("Refusing to use same field twice.")
            d[key] = []
            line = line[1]

        line = line.strip()
        if line:
            # line continuation (or the first line after field name stripped away)
            if key is None:
                raise Exception("Expected field name ( * Something: ...).")
            d[key].append(line)

    return {k: "\n".join(v) for k, v in d.items()}


def preprocess(text):
    header = parse_header(text)
    if header is None:
        header = {}

    # Remove /** ... */ and /// ... comments (including excess whitespace that could remain)
    text = re.sub(r" */\*\*.*?\*/\n?", "", text, flags=re.DOTALL | re.MULTILINE)
    text = re.sub(r"\n? *///.*", "", text, flags=re.MULTILINE)
    text = re.sub(r"^\s*\#pragma once", "", text, flags=re.DOTALL).strip()
    text = re.sub(r'^\s*\#include "\.\./base.hpp"', "", text, flags=re.DOTALL).strip()
    return header, text


main()
